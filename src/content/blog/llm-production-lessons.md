---
title: What I Learned Shipping LLM Features to Production
description: The unglamorous parts of shipping LLM features: demo vs prod, prompts-as-code, latency, fallbacks, and when not to use AI.
date: 2025-06-10
tags: [AI, Engineering]
featured: false
---

I spent the last year shipping Text-to-SQL features powered by LLMs. The demo is easy: type a question, get a chart. Production is where it gets weird.

Here's what actually matters when an LLM feature has to behave like a product, not a demo.

## The demo → production gap is enormous

Demo: "Show me revenue by month"
→ Works perfectly, everyone claps

Production:
- "can u show rev monthly?" — typos, abbreviations
- "Show me the thing we looked at yesterday" — context you don't have
- "Revenue but only for products we still sell" — business logic the model doesn't know
- "更多收入数据" — users who don't speak English

It’s the same in music: a mix can sound huge in your headphones and collapse on a real PA. Demos are headphones. Production is the venue.

Every edge case in demos becomes the common case in production. Plan for it.

## Prompt engineering is software engineering

Early on, I treated prompts as magic strings. That doesn't scale.

Now I treat them like code:

```typescript
const buildAnalyticsPrompt = ({
  schema,
  userQuery,
  previousContext,
  businessRules
}: PromptParams) => `
You are an analytics assistant for ${schema.companyName}.

Available tables:
${formatSchema(schema)}

Business rules:
${businessRules.map(r => `- ${r}`).join('\n')}

Previous context:
${previousContext || 'None'}

User request: ${userQuery}

Generate a SQL query that...
`;
```

Prompts should be:
- Parameterized
- Version controlled
- Tested
- Reviewed like any other code
- Observable (logged with version, model, and inputs)

## Fallbacks are not optional

LLMs fail. Not sometimes — regularly. You need:

1. **Input validation** — Catch obviously bad queries before wasting API calls
2. **Timeout handling** — Set aggressive timeouts, have fallbacks
3. **Confidence scoring** — If the model isn't sure, don't pretend it is
4. **Graceful degradation** — Show a helpful error, not a stack trace
5. **Human escalation** — Let users report issues, actually read the reports
6. **Telemetry + replay** — Capture inputs/outputs so you can debug and build evaluation datasets

Our error rate dropped 60% when we added a simple "I'm not sure I understood that. Did you mean X, Y, or Z?" response for low-confidence outputs.

## Latency is a feature

GPT-4 is smarter than GPT-3.5-turbo. It's also 5x slower. For many use cases, speed matters more than marginal intelligence improvement.

We run a two-tier system:
- Fast model for simple queries (70% of traffic)
- Smart model for complex ones (30% of traffic)
- Classification model decides which tier to use

Average latency dropped from 4s to 1.2s. User satisfaction went up more than when we improved accuracy.

## Context windows are for data, not instructions

With 128k context windows, it's tempting to dump everything in. Don't.

**Bad pattern:**
```
Here are all our tables, all columns, all business rules,
all previous conversations, all documentation...
```

**Better pattern:**
```
Here are the 3 tables relevant to this query, their key columns,
and 2 business rules that apply.
```

Focused context = better outputs + faster responses + lower costs.

## Users don't want AI, they want answers

This sounds obvious but it's easy to forget. Nobody cares that you're using GPT-4o. They care whether they can get their revenue report.

We removed all mentions of "AI" from the UI. It's just a search box now. No "AI-powered!" badges. No "Generated by AI" disclaimers on every output.

Users trust it more because we don't draw attention to the technology.

## Measure what matters

We tracked:
- Query success rate
- Result accuracy (sampled and human-verified)
- User satisfaction (thumbs up/down)
- Time to insight
- Retry rate

We didn't track:
- How "impressive" the AI responses sounded
- How many features used AI
- Lines of prompt code

The first list tells you if you're solving user problems. The second is vanity metrics.

## Know when to not use AI

Some things don't need LLMs:
- Queries that map directly to a known template → use the template
- Ambiguous requests that need clarification → just ask the user
- High-stakes decisions → require human confirmation
- Anything with existing reliable solutions → don't over-engineer

We added AI to 6 features. We removed it from 2 after realizing simpler approaches worked better.

## The tech is moving faster than you can ship

Every quarter, better models come out. Prompts that worked perfectly break. Costs change. New capabilities emerge.

Design for change:
- Abstract the LLM provider
- Make prompts configurable without deploy
- Build evaluation suites you can run against new models
- Don't bet everything on one provider's specific features

## It's worth it

Despite all the challenges, the LLM features we shipped are our most-loved. Users who never wrote SQL now build their own dashboards. That's the point.

The technology is imperfect, but it unlocks capabilities that weren't possible before. That's why we deal with the complexity.

---

Building AI features yourself? I'd love to hear what challenges you're hitting.
